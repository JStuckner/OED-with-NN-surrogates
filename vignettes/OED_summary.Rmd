---
title: "R Notebook"
output: html_notebook
---

Optimal Experimental Design (OED) is a statistical technique geared towards optimizing controllable design parameters to maximize the value of experiments performed, and the data generated by them. A direct result of this approach is illustrated through the following scenario: assume, due to time and financial constraints, that one can only afford to run $n$ experiments of given type of test. The test has several controllable parameters (called design parameters), which can be varied at the time of the test, and several uncontrollable variables, which are typically assumed to be random variables. The goal is of the tests is, generally, to quantify or further understand some quantity of interest $\gamma = (\gamma_1, \gamma_2, \dots, \gamma_p)^\intercal$ which may be, e.g. the response variables or outcomes of a particular model. Generally, one has collected data $\boldsymbol{X}_n = \{x_1, x_2, \dots, x_n\} \text{ where } \forall i \in [1, n], x_i \in  \mathbb{R}^d$, and is interested in optimizing a set of design parameters $\boldsymbol{D} \subset \mathbb{R}^{n \times d}$, an $n \times d$ matrix where each row represents a combination of controllable design parameters, generally assumed to be used to produce $x_i$. $nd$ is sometimes referred to as the dimensionality of the design.

Utility, as originally expressed by Lindeley, is often expressed as follows: 

$$ U(\eta) = \int_{\mathcal{Y}} \int_{\Theta} u(\eta, y, \theta) p(\theta, y \mid \eta) d\theta dy$$
where $\nu$ is a vector of design parameters, $y$ is the observed data, and $\theta$ are the model parameters. 
The utility $u$ is most often a scalar function of the Fisher information matrix. When the objective of the experiment is parameter inference, a useful utility function to consider might the following:

$$ u(\eta, y, \theta)= u(\eta, y) = D_{\mathrm{KL}}(p(\theta, y \mid \eta) \;\|\; p(\theta) )$$
Taking the expected value of this utility function over the prior predictive of the data is equivalent to the mutual information between the data and the parameters, $I(y; \theta)$. 

If the model is sufficiently complex (and non-linear), the model parameters may not always have a straightforward physical interpretation. Highly overparameterized (and successful) neural architectures, such as convolutional neural networks, are exemplary cases of this. 

Instead, we consider the model parameters $\theta$ as independent of the design parameters $\eta$, and instead consider the following utility: 

$$u(\eta, y, \theta) = u(\eta, y) = D_{\mathrm{KL}}(p(Q\mid y, \eta) \;\|\; p(Q))$$
Where $Q$ is a predictive quantity of interest, $p(Q\mid y, \eta) = \int p(Q \mid \theta)p(\theta \mid y, \eta) d\theta$, and $p(Q) = \int p(Q\mid\theta)p(\theta)d\theta$. Taking expectation of this utility over the data yields $U(\eta) = I(Q; y \mid \eta)$; the mutual information between the predictions $Q$ and the data $y$, conditioned on the design $\eta$. Thus, this functional implicitly defines a information-theoretic version of a ``forward'' sensitivity analysis. The motivation is that evaluations such an analysis (i.e. the 'optimal' design parameters settings to use for improving predictive accuracy of the quantity $Q$) enables a statistical interpretation of how much each input is contributing to the output uncertainty, towards building a better understanding of how the exact physical relationships between the inputs affect the variability of the outputs. 
Theoretically, the $u$ above is defined as follows: 

$$\int_{Q} p(Q\mid y, \eta) \log{\frac{p(Q\mid y, \eta)}{p(Q)}} dQ$$

$$I(Q; y \mid \eta) = \int_{Q}\int_{\mathcal{Y}} $$

A decision-theoretic Bayesian optimal design $d^*$ maximizes the *expected utility* of a particular design, i.e. 

$$
\begin{split}
U(\boldsymbol{D}) & = \mathbb{E}_{\gamma, y \mid D}[u(\gamma, y, D)] \\
& = \int u(\gamma, y, D) p(\gamma \mid y, D) p(y \mid D) d\gamma dy \\
& = \int u(\gamma, y, D) p(y \mid \gamma, D) p(\gamma \mid D) d\gamma dy
\end{split}
$$



Thus, OED enables the efficient allocation of resources such that the corresponding design of experiments is 'optimal' in some sense. The formal expression of the utility function determines this optimality criterion, and depends on the aim of the experiment.

With the exception of Gaussian-Markov (linear) models, evaluating this utility function is analytically intractable. If certain assumptions are taken into account, i.e. a parametric model is carefully constructed, then there are several out-of-the-box utility functions that may be used.

For nonparametric, higher dimensional optimal Bayesian designs, I rely on the the approximate coordinate exchange (ACE) algorithm from the package `acebayes`.

To specify the optimal design, one needs to specify: 
1. The statistical model ($a$)
2. The prior distribution ($b$)
3. Approximation of the utility function ($c$)

Overstall et. al emphasize: "Prior to application of the ACE algorithm, a relevant, and perhaps pragmatic, choice of utility function must be made that encapsulates the aim of the experiment."

```{r}
utilfisher <- function(d, B) {
  theta <- rnorm(B)
  ui <- matrix(rep(d[, 1] ^ 2, B), ncol = B) * exp(outer(d[, 1], theta))
  apply(ui, 2, sum)
}

n <- 35
start.d <- matrix(0, nrow = n, ncol = 1)
opt_design <- acebayes::ace(utilfisher, start.d = start.d)

mean(opt_design$utility(d = opt_design$phase1.d, B = 20000))

```


```{r}
## Stochastic utility function
utility <- function(d, B){
  
}
```



For example, consider the objective (page 4-5 of that paper): 
$$
\begin{equation}
  u(\eta, Y, \theta) = D_{KL}(p(Q|y, \eta) || p(Q))
\end{equation}
$$

where:
 
1. $$ D_{KL}(P || Q) := \int\limits_{-\infty}^{\infty} p(x) \log{\frac{p(x)}{q(x)}} $$
2. $$ p(Q | y, \eta) := \int p(Q | \theta) p(\theta | y, \eta) d\theta $$
3. $$ p(Q) := \int p(Q | \theta) p(\theta) d\theta $$


## Summary of the approach 

In summary, the goal is to build a faithful, computationally inexpensive surrogate model which mimicks a given physical simulation of an experiment. The design space is carefully chosen to encompass the aim of the analysis. 

The goal is maximize the expected utility: 

$$ U(\eta) = \int_{\mathcal{Y}} \int_{Q} u(\eta, y, Q) p(Q, y \mid \eta) dQ dy $$
where: 
$$ Q := < \text{quantity to predict} >$$
$$ \eta := < \text{design parameters} >$$
$$ Y := < \text{data observed from experiment} >$$
In order to do this, a correspondence between what MAC/GMC can provide and these quantities need to be established. It's assumed that $Q$ would be some thermo-elastic property of interest (i.e. UTS, E, etc.), $\eta$ would be the set of controllable parameters which need to be chosen before performing the experiment (i.e. fiber/matrix volume fraction, properties of the laminate, etc.), and $y$ would be other things observed as output from GMC other than $Q$ (i.e. laminate stiffness coefficients?). All other parameters associated with the simulation are assumed fixed (i.e. the type of materials making up the laminate). 

1) Pick the set of design parameters which are not fixed.  
2) Pick the hyper-parameters to vary the parameters from (1) according to known distributions. 
3) Run the physical simulation (MAC/GMC) with the generated design parameters from (2).
4) Train a surrogate model (ANN) to mimick the physical simulation:
  4a) Performing (4) reduces to choosing an output property to predict (e.g. UTS), based on some inputs (e.g. $\eta$, $y$). An artificial neural network gives likelihood $p(Q \mid y, \eta)$ 


## Creating the physical simulation 

To create the full surrogate model, the ANN needs a sufficient amount of data to mimic the physical simulation $G$. testing materials is financially expensive. As a proxy for physical experimentation, we rely on a high-fidelity model which can simulate various materials responses to a sufficiently high detail. In particular, the software MAC/GMC is used to simulate a material stress response under a fixed load. The local stress fields are observed from the simulation, which we conceptually think of as the output of the experiment. The resulting, observed thermo-elastic properties extracted from the simulation are the quantities we wish to model, given the local stress field information and the design parameters. Given a sufficiently trained model, the goal is to understand the intrinsic relationship these local, lower-length properties have on the given thermo-elastic property of interest. 
