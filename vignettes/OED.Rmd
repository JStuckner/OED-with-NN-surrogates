---
title: "Optimal Experimental Design"
output: html_notebook
header-includes: 
  - \usepackage{mathtools}
---

Optimal Experimental Design (OED) is a statistical technique geared towards optimizing controllable design parameters to maximize the value of experiments performed, and the data generated by them. A direct result of this approach is illustrated through the following scenario: assume, due to time and financial constraints, that one can only afford to run $n$ experiments of given type of test. The test has several controllable parameters (called design parameters), which can be varied at the time of the test, and several uncontrollable variables, which are typically assumed to be random variables. The goal is of the tests is, generally, to quantify or further understand some quantity of interest $\gamma = (\gamma_1, \gamma_2, \dots, \gamma_p)^\intercal$ which may be, e.g. the response variables or outcomes of a particular model. Generally, one has collected data $\boldsymbol{X}_n = \{x_1, x_2, \dots, x_n\} \text{ where } \forall i \in [1, n], x_i \in  \mathbb{R}^d$, and it interested in optimizing a set of design parameters $\boldsymbol{D} \subset \mathbb{R}^{n \times d}$, an $n \times d$ matrix where each row represents a combination of controllable design parameters, generally assumed to be used to produce $x_i$. $nd$ is sometimes referred to as the dimensionality of the design.

Utility, as originally expressed by Lindeley, is often expressed as follows: 

$$ U(\eta) = \int_{\mathcal{Y}} \int_{\Theta} u(\eta, y, \theta) p(\theta, y \mid \eta) d\theta dy$$
where $\nu$ is a vector of design parameters, $y$ is the observed data, and $\theta$ are the model parameters. 
The utility $u$ is most often a scalar function of the Fisher information matrix. When the objective of the experiment is parameter inference, a useful utility function to consider might the following:

$$ u(\eta, y, \theta)= u(\eta, y) = D_{\mathrm{KL}}(p(\theta, y \mid \eta) \;\|\; p(\theta) )$$
Taking the expected value of this utility function over the prior predictive of the data. Thus, this is equivalent to the mutual information between the data and the parameters, $I(y; \theta)$. 

If the model is sufficiently complex (and non-linear), the model parameters may not always have a clear, interpretable meaning. The highly overparamterized deep neural networks, such as convolution neural networks, are exemplary cases of this. 

Instead, we consider the model parameters $\theta$ as independent of the design parameters $\eta$, and instead consider the following utility: 

$$u(\eta, y, \theta) = u(\eta, y) = D_{\mathrm{KL}}(p(Q\mid y, \eta) \;\|\; p(Q))$$
Where $Q$ is a predictive quantity of interest, $p(Q\mid y, \eta) = \int p(Q \mid \theta)p(\theta \mid y, \eta) d\theta$, and $p(Q) = \int p(Q\mid\theta)p(\theta)d\theta$. Taking expectation of this utility over the data yields $U(\eta) = I(Q; y \mid \eta)$; the mutual information between the predictions $Q$ and the data $y$, conditioned on the design $\eta$. Thus, this functional implicitly defines a information-theoretic version of a ``forward'' sensitivity analysis. The motivation is that evaluations such an analysis (i.e. the 'optimal' design parameters settings to use for improving predictive accuracy of the quantity $Q$) enables a statistical interpretation of how much each input is contributing to the output uncertainty, towards building a better understanding of how the exact physical relationships between the inputs affect the variability of the outputs. MLP' expects 'Y' to be an integer vector of classes.") }
  
  set.seed(1234) ## for reproducibility
  if (is(Y, "integer")){
  ## One-hot encoding assumes 0-based class labels
    Y <- match(Y, unique(Y)) - 1L

Theoretically, the $u$ above is defined as follows: 

$$\int_{Q} p(Q\mid y, \eta) \log{\frac{p(Q\mid y, \eta)}{p(Q)}} dQ$$

$$I(Q; y \mid \eta) = \int_{Q}\int_{\mathcal{Y}} $$

A decision-theoretic Bayesian optimal design $d^*$ maximizes the *expected utility* of a particular design, i.e. 

$$
\begin{split}
U(\boldsymbol{D}) & = \mathbb{E}_{\gamma, y \mid D}[u(\gamma, y, D)] \\
& = \int u(\gamma, y, D) p(\gamma \mid y, D) p(y \mid D) d\gamma dy \\
& = \int u(\gamma, y, D) p(y \mid \gamma, D) p(\gamma \mid D) d\gamma dy
\end{split}
$$



Thus, OED enables the efficient allocation of resources such that the corresponding design of experiments is 'optimal' in some sense. The formal expression of the utility function determines this optimality criterion, and depends on the aim of the experiment.

With the exception of Gaussian-Markov (linear) models, evaluating this utility function is analytically intractable. If certain assumptions are taken into account, i.e. a parametric model is carefully constructed, then there are several out-of-the-box utility functions that may be used.

For nonparametric, higher dimensional optimal Bayesian designs, I rely on the the approximate coordinate exchange (ACE) algorithm from the package `acebayes`.

To specify the optimal design, one needs to specify: 
1. The statistical model ($a$)
2. The prior distribution ($b$)
3. Approximation of the utility function ($c$)

Overstall et. al emphasize: "Prior to application of the ACE algorithm, a relevant, and perhaps pragmatic, choice of utility function must be made that encapsulates the aim of the experiment."

```{r}
utilfisher <- function(d, B) {
  theta <- rnorm(B)
  ui <- matrix(rep(d[, 1] ^ 2, B), ncol = B) * exp(outer(d[, 1], theta))
  apply(ui, 2, sum)
}

n <- 35
start.d <- matrix(0, nrow = n, ncol = 1)
opt_design <- acebayes::ace(utilfisher, start.d = start.d)

mean(opt_design$utility(d = opt_design$phase1.d, B = 20000))

```


```{r}
## Stochastic utility function
utility <- function(d, B){
  
}
```



For example, consider the objective (page 4-5 of that paper): 
$$
\begin{equation}
  u(\eta, Y, \theta) = D_{KL}(p(Q|y, \eta) || p(Q))
\end{equation}
$$

where:
 
1. $$ D_{KL}(P || Q) := \int\limits_{-\infty}^{\infty} p(x) \log{\frac{p(x)}{q(x)}} $$
2. $$ p(Q | y, \eta) := \int p(Q | \theta) p(\theta | y, \eta) d\theta $$
3. $$ p(Q) := \int p(Q | \theta) p(\theta) d\theta $$


## Summary of the approach 

In summary, the goal is to build a faithful, computationally inexpensive surrogate model which mimicks a given physical simulation of an experiment. The design space is carefully chosen to encompass the aim of the analysis. 

The goal is maximize the expected utility: 

$$ U(\eta) = \int_{\mathcal{Y}} \int_{Q} u(\eta, y, Q) p(Q, y \mid \eta) dQ dy $$
where: 
$$ Q := < \text{quantity to predict} >$$
$$ \eta := < \text{design parameters} >$$
$$ Y := < \text{data observed from experiment} >$$
In order to do this, a correspondence between what MAC/GMC can provide and these quantities need to be established. It's assumed that $Q$ would be some thermo-elastic property of interest (i.e. UTS, E, etc.), $\eta$ would be the set of controllable parameters which need to be chosen before performing the experiment (i.e. fiber/matrix volume fraction, properties of the laminate, etc.), and $y$ would be other things observed as output from GMC other than $Q$ (i.e. laminate stiffness coefficients?). All other parameters associated with the simulation are assumed fixed (i.e. the type of materials making up the laminate). 

1) Pick the set of design parameters which are not fixed.  
2) Pick the hyper-parameters to vary the parameters from (1) according to known distributions. 
3) Run the physical simulation (MAC/GMC) with the generated design parameters from (2).
4) Train a surrogate model (ANN) to mimick the physical simulation:
  4a) Performing (4) reduces to choosing an output property to predict (e.g. UTS), based on some inputs (e.g. $\eta$, $y$). An artificial neural network gives likelihood $p(Q \mid y, \eta)$ 


## Creating the physical simulation 

To create the full surrogate model, the ANN needs a sufficient amount of data to mimic the physical simulation $G$. testing materials is financially expensive. As a proxy for physical experimentation, we rely on a high-fidelity model which can simulate various materials responses to a sufficiently high detail. In particular, the software MAC/GMC is used to simulate a material stress response under a fixed load. The local stress fields are observed from the simulation, which we conceptually think of as the output of the experiment. The resulting, observed thermo-elastic properties extracted from the simulation are the quantities we wish to model, given the local stress field information and the design parameters. Given a sufficiently trained model, the goal is to understand the intrinsic relationship these local, lower-length properties have on the given thermo-elastic property of interest. 

To do this, we employ the use of information-theoretic measures to quantify these relationships.

```{r}
## Layups to test 
layups <- c("[0]_24", 
            "[90]_24", 
            "[0/90]_12", 
            "[0/90]_6S", 
            "[15, -15]_12", 
            "[30, -30]_12",
            "[45, -45]_12",
            "[60, -60]_12", 
            "[75, -75]_12",
            "[15, -15]_6S",
            "[30, -30]_6S",
            "[45, -45]_6S",
            "[60, -60]_6S",
            "[75, -75]_6S",
            "[0, -45, 45, 90]_3S", 
            "[0, -60, 60]_4S", 
            "[0, 90, -30, 30, -60, 60]_2S", 
            "[0, 45, 90, -45]_6", 
            "[45, -45, 90, 0]_3S") 
for (li in 1:length(layups)){ assign(paste0("lam", li), value = makeLayup(layups[[li]])) }
lam_layups <- sapply(1:length(layups), function(i) eval(as.symbol(sprintf("lam%d", i))))
```

Generate the MAC files. 

```{r}
# Simulating graphite and epxoy laminates
sapply(list.files("~/GENERATED_MAC/"), function(fn) file.remove(file.path("~/GENERATED_MAC", fn)))
cc <- 1L
for (i in 1:500){

  ## Graphite fiber 
  mat1 <- list(M=1, CMOD=6, MATID="U", MATDB=1)
  
  E_A <- rnorm(n = 1, mean = 388200, sd = 388200*0.03)
  fiber_str <- rnorm(n = 1, mean = 3500, sd = 3500*0.05)
  
  E_Am <- rnorm(n = 1, mean = 3450, sd = 3450*0.03)
  matrix_str <- rnorm(n = 1, mean = 80, sd = 80*0.05)
  
  G_A <- rnorm(n = 1, mean = 14900, sd= 14900*0.03)
  V <- E_A/2*G_A -1 
  mat1 <- append(mat1, list(EL=c(E_A,E_A,V,V,G_A,-0.68,9.74))) # moduli in MPa
  
  E_Am <- rnorm(n = 1, mean = 3450, sd = 3450*0.03)
  G_Am <- rnorm(n = 1, mean = 1280, sd = 1280*0.03)
  Vm <- E_Am/2*G_Am -1 
  mat2 <- list(M=2, CMOD=6, MATID="U", MATDB=1)
  mat2 <- append(mat2, list(EL=c(E_Am,E_Am,Vm,Vm,G_Am,45,45))) # moduli in MPa
  
  #   ## Material 1 (Fiber)
#   mat1$E_A <- rnorm(n = 1, mean = 388200, sd = 388200*0.03)
#   mat1$G_A <- rnorm(n = 1, mean = 14900, sd= 14900*0.03)
#   sc1$X11 <- rnorm(n = 1, mean = 3500, sd = 3500*0.05) # in Mega pascals
#   sc1$XC11 <- sc1$X11
#   
#   ## Material 2 (Matrix)
#   mat2$E_A <- rnorm(n = 1, mean = 3450, sd = 3450*0.03)
#   mat2$G_A <- rnorm(n = 1, mean = 1280, sd = 1280*0.03)
#   sc2$X11 <- rnorm(n = 1, mean = 80, sd = 80*0.05)
  
  ## Epoxy Matrix 
  # mat2 <- list(M=2, CMOD=6, MATID="U", MATDB=1)
  # mat2 <- append(mat2, list(EL=c(3450,3450,0.35,0.35,1278,-45.E-6,45E-6))) # moduli in MPa
  #   
  ## Form the constituents section
  constituents <- list("*CONSTITUENTS", "  NMATS=2", paste0("  ", parse_row(mat1)), paste0("  ", parse_row(mat2)))
  
  for (li in 1:length(layups)){
    clam <- eval(as.symbol(paste0("lam", li)))
    
    ## Form simple laminate
    n_ply <- 24
    lam_vf <- rnorm(n_ply, mean = 0.60, sd = 0.60*0.03)
    cangles <- rnorm(length(clam), mean = clam, sd = abs(clam*0.01))
    lam_def <- data.frame(LY=1:n_ply, MOD=2, THK=1/n_ply, ANG=cangles, ARCHID=1, VF=lam_vf, "F"=1, M=2)
    laminate <- append(list("*LAMINATE", sprintf("  NLY=%d", n_ply)), as.list(paste0("   ", parse_df(lam_def))))
    
    ## Apply a fixed mechanical load
    mech <- data.frame(NPT=rep(2L, 6), TI=rep("0.,1.", 6L), MAG=c("0.,0.05", rep("0.,0.", 5L)), 
                       MODE=c(1, rep(2, 5)))
    mload <- append(list("*MECH", "  LOP=99"), as.list(parse_df(mech, pre_space = "  ")))
    
    ## Static failure analysis 
    criteria <- "   CRIT=%d X11=%f COMPR=SAM ACTION=1"
    sfa <- list("*FAILURE_SUBCELL", " NMAT=2", 
                "  MAT=1 NCRIT=1", sprintf(criteria, 1, fiber_str), 
                "  MAT=2 NCRIT=1", sprintf(criteria, 1, matrix_str))
    
    ## Solver 
    solver_settings <- list(METHOD=1, NPT=2, TI="0., 1.", STP=0.001, ITMAX=20, ERR=1e-4)
    solver <- list("*SOLVER", paste0("  ", parse_row(solver_settings)))
    
    ## Output section
    out <- list("*PRINT", "  NPL=3")
    
    ## Get the stress strain output as well 
    ss_out <- list("*XYPLOT", "  FREQ=1", "  LAMINATE=1", 
                   sprintf("  NAME=graphite_epoxy_ss%d X=1 Y=10", cc), 
                   "  MACRO=0", "  MICRO=0")
    
    ## Generate test mac files
    mac_file <- c(constituents, laminate, mload, sfa, solver, out, ss_out, "*END")
    readr::write_lines(mac_file, path = sprintf("~/GENERATED_MAC/graphite_epoxy_%d.mac", cc))
    cc <- cc + 1L
  }
}

```

Run MAC/GMC 

```{r}
library("MaterialAnalysis")
# Sys.setenv("pass"="...")
runMAC(src_dir = "~/GENERATED_MAC", dest_dir = "/acp/u/mpiekenb/testing", base_fn = "graphite_epoxy",
       user = "mpiekenb", pass = Sys.getenv("pass"))
```

After running the example laminates through the Generalized Method of Cells algorithm with MAC/GMC, the resulting lower-length scale information is extracted, along with their corresponding thermo-elastic properties extracted from the recorded stress strain curve. 

```{r}

## (in sequences of 100 files at a time) run: 
## scp ~/GENERATED_MAC/* mpiekenb@cryoterm1:~/testing/ 
## ssh cryoterm1
## cd testing
## sbatch sbatch_script.sh
## scp mpiekenb@cryoterm1:~/testing/* ~/GENERATED_MAC/
## system("scp ~/GENERATED_MAC/test_mac.mac mpiekenb@cryoterm1:~/testing/ ")
## Plots stress strain lines
# library("latex2exp")
# avg_fv <- sapply(mac_result, function(mf) mean(mf[["section II"]][, 2]))
# col_pal <- rev(rainbow(100, start = 0, end = 4/6))
# col_pal <- adjustcolor(col_pal, alpha.f = 0.20)
# col_idx <- findInterval(avg_fv, vec = quantile(avg_fv, seq(0.01, 1, 0.01)), rightmost.closed = TRUE, all.inside = TRUE)
# params <- list(xlab="Strain", ylab="Stress")
# params <- append(params, list(main="GMC Laminate Variation")) 
# #TeX("Fiber VF $\\sim\\;N(\\mu = 0.5, sd = 0.01)$"
# params <- append(params, list(x = ss_result[[1]], type = "l", col = col_pal[[col_idx[[1]]]]))
# params <- append(params, list(ylim = c(0, max(ss_stats$ult_tensile_stress))))
# do.call(plot, params)
# sapply(2:n_files, function(i) lines(ss_result[[i]], col = col_pal[[col_idx[[i]]]]))

```

## Building the surrogate model  

Varying the experimental parameters along know defaults is the first part of the problem. Collecting the results of the experiments, $y$, along with the thermo-elastic observations of interest, $Q$, is the second part. Given these results, the goal now is to build a computationally inexpensive model $G$ that sufficiently mimicks the GMC. For this task, we employ a neural network trained with tensorflow. 

Specifically, the goal is to model the joint density of the experimental outcomes, given a design. That is, the goal is sufficiently model: 

$$ p(y, Q \mid \eta) $$
Such that we can draw samples from the joint distribution, $p(y_b, Q_b \mid \eta)$


So approximating the expected utility requires hundreds of thousands, if not millions of evaluations of UTS. This is well beyond the capability of the current software implementation of GMC for sufficiently complex loads or laminates. Instead, we build a surrogate model using Deep Learning to mimic the generalized method of cells procedure for this specific laminate. That is, for a given laminate built from a composite with fixed fiber and matrix elastic properties, we vary the orientations of the laminates. 

```{r}
## Get ready to parse the outputs
res_files <- list.files("~/GENERATED_MAC/")

## Get MAC input properties 
res_mac <- pbapply::pblapply(res_files[grep(res_files, pattern = "*.mac")], function(mac_file){
  parseMAC(file_path = normalizePath(file.path("~/GENERATED_MAC", mac_file)))
})

## Get the resulting outputs from GMC
res_out <- pbapply::pblapply(res_files[grep(res_files, pattern = "*.out")], function(out_file){
  parseOUT(file_path = normalizePath(file.path("~/GENERATED_MAC", out_file)), sections = 2L)
})

## Get the stress strain curves
res_data <- pbapply::pblapply(res_files[grep(res_files, pattern = "*.data")], function(data_file){
  parseDATA(file_path = normalizePath(file.path("~/GENERATED_MAC", data_file)))
})

## Get the inferred stress-stress properties
res_ss <- pbapply::pblapply(res_data, function(ss){
  getStressStrainStats(strain = ss[, 1], stress = ss[, 2])
})
res_ss_props <- data.table::rbindlist(lapply(res_ss, as.list))

# experiment1 <- list(res_mac, res_out, res_data, res_ss_props)
# save(experiment1, file = "~/experiment1.rdata")
# rm(experiment1)

## load the inputs 
load("~/experiment1.rdata")
res_mac <- experiment1[[1]] 
res_out <- experiment1[[2]]
res_data <- experiment1[[3]]
res_ss_props <- experiment1[[4]]
```

Building a model to predict properties along the stress strain curve

```{r}
## Load components needed to build a neural net
source('~/grc-materials-analysis/MaterialAnalysis/R/NN.R')

## Normalize closure
normalizer <- function(x, a = -1, b = 1, rng=NULL){ 
  # if (!is.null(opts) list("ab_range"=list(a=0, b=1), =list(center=TRUE, scale=TRUE))
  if (is.vector(x)){
    { min_x <- min(x); max_x <- max(x) }
    normalize <- function(x, invert=FALSE){ 
      if (missing(invert) || !invert){ return(((b - a)*(x - min_x)) / (max_x - min_x) + a ) }
      else { return(((x - a)/(b - a))*(max_x - min_x) + min_x) }
    }
    return(normalize)
  }
  else if (is.matrix(x)){
    d_x <- ncol(x)
    if (missing(rng) || is.null(rng)){ 
      col_idx <- as.list(seq(ncol(x)))
      x_rng <- apply(x, 2, range)
    } else { 
      if (sum(rng) != d_x) { stop("'rng' should sum to the number fo columns in x.")}
      start_idx <- cumsum(rng) - rng + 1
      col_idx <- lapply(1:length(rng), function(i) start_idx[i]:(start_idx[i]+rng[i]-1L))
      x_rng <- sapply(1:length(col_idx), function(i) { range(as.vector(unlist(x[, col_idx[[i]]]))) })  
    }
    normalize <- function(x, invert=FALSE){ 
      if (missing(invert) || !invert){ 
        x_norm <- matrix(0, ncol = ncol(x), nrow = nrow(x))
        for (i in 1:length(col_idx)){
          idx <- col_idx[[i]]
          x_tmp <- as.vector(unlist(x[, idx]))
          x_norm[, idx] <- ((b - a)*(x_tmp - x_rng[1, i])) / (x_rng[2, i] - x_rng[1, i]) + a 
        }
        return(x_norm) 
      }
      else {
        x_unnorm <- matrix(0, ncol = ncol(x), nrow = nrow(x))
        for (i in 1:length(col_idx)){
          idx <- col_idx[[i]]
          x_tmp <- as.vector(unlist(x[, idx]))
          x_unnorm[, idx] <- (((x_tmp - a)/(b - a))*(x_rng[2, i] - x_rng[1, i]) + x_rng[1, i]) 
        }
        return(x_unnorm)
      }
    }
  }
  return(normalize)
}

## Inputs (constituent properties, fiber volume fractions [per ply], layup, and ABD components)
fiber_props <-  do.call(rbind, lapply(res_mac, function(mac) mac$fiber_constituent_properties[c(1, 3, 8)]))
matrix_props <-  do.call(rbind, lapply(res_mac, function(mac) mac$matrix_constituent_properties[c(1, 3, 8)]))
fiber_vf <- do.call(rbind, lapply(res_mac, function(mac) mac$laminate_properties$VF))
orientations <- do.call(rbind, lapply(res_mac, function(mac) mac$laminate_properties$ANG))
ABD <- do.call(rbind, lapply(res_out, function(rout) unlist(rout[["section II"]]$ABD_matrix)))


## Separate out the laminates by the layup 
layup_class <- sapply(res_mac, function(mac){
  which.min(apply(lam_layups, 2, function(lu) sum((lu - mac$laminate_properties$ANG)^2))) 
})
layup_one_hot <- to_categorical(y = layup_class - 1L, num_classes = length(layups))


## Choose a layup to analyze 
# c_layup <- 1:19
# c_layup_idx <- which(layup_class == c_layup)
# X_tmp <- cbind(fiber_props[c_layup_idx,], 
#                matrix_props[c_layup_idx,], 
#                fiber_vf[c_layup_idx,], 
#                # orientations[c_layup_idx, ], 
#                ABD[c_layup_idx,])
# X_normalizer <- normalizer(X_tmp)
# X <- X_normalizer(X_tmp)
# X[is.na(X)] <- 0
X_tmp <- cbind(fiber_props, matrix_props, fiber_vf, orientations, ABD)
X_normalizer <- normalizer(X_tmp,
                           rng = c(rep(1, ncol(fiber_props) + ncol(matrix_props)),
                                   ncol(fiber_vf), ncol(orientations), ncol(ABD)))
X <- cbind(layup_one_hot, X_normalizer(X_tmp)) ## ensure any(is.na(X)) == FALSE

## Outputs (properties along the stress strain curve)
# c_ss_props <- as.matrix(res_ss_props[c_layup_idx,])
c_ss_props <- as.matrix(res_ss_props)
Y_normalizer <- normalizer(c_ss_props)
Y <- Y_normalizer(c_ss_props)

## Configure the inputs 
X <- data.table::data.table(X)
data_splits <- partition(X = X, Y = Y, input_shape = c(ncol(X)), 
                         splits = c(train = 0.70, validate = 0.20, test = 0.10))

## The model layers
decision_layers <- list(
  "dense" = list(units = 25, activation = 'tanh'),
  "dropout" = list(rate = 0.35),
  "dense" = list(units = 45, activation = 'tanh'),
  "dropout" = list(rate = 0.35),
  "dense" = list(units = 45, activation = 'tanh'),
  "dropout" = list(rate = 0.35),
  "dense" = list(units = 25, activation = 'tanh'),
  "dropout" = list(rate = 0.35),
  "dense" = list(units = ncol(Y), activation = 'linear')
)

## Generate the layer tensors
keras::k_clear_session()
main_input <- layer_input(shape = c(ncol(X)), name = 'main_input')

## Connect the inputs
dense_net <- MLP(layers = decision_layers)
predictions <- main_input %>% dense_net

## Make the model
model <- keras_model(inputs = main_input, outputs = predictions)

## Compile the model with the selected loss and optimizer
model %>% compile(
  loss = 'mean_squared_error',
  # optimizer = optimizer_rmsprop(),
  optimizer = optimizer_nadam(),
  metrics = c('mae')
)

## Train the model
history <- model %>% fit(
  x = data_splits$train$X,
  y = data_splits$train$Y, 
  epochs = 250, batch_size = 30, 
  # validation_split = 0.20
  validation_data = list(data_splits$val$X, data_splits$val$Y)
)

keras::save_model_hdf5(model, filepath = "~/nn_model2")
model <- keras::load_model_hdf5("~/nn_model2")

## See how it performs on the different partitions of the data
model %>% evaluate(data_splits$train$X, data_splits$train$Y)
model %>% evaluate(data_splits$val$X, data_splits$val$Y)
model %>% evaluate(data_splits$test$X, data_splits$test$Y)

```

Getting the predictions, scaling back to the original units
```{r}
# ## Invert the normalization
# ss_predicted_norm <- predict(model, as.matrix(X)) 
# ss_predicted <- Y_normalizer(ss_predicted_norm, invert = TRUE)
# 
# ## Have eta include the average fiber VF
# design_vars <- cbind(fiber_props, matrix_props, apply(fiber_vf, 1, mean), layup_class)
# eta_normalizer <- normalizer(design_vars)
# abd_normalizer <- normalizer(ABD)
# layup_normalizer <- normalizer(layup_class)
# layup_orientation_normalizer <- normalizer(as.vector(unlist(lam_layups)))
# 
# ## The three variables needed for OED
# eta <- eta_normalizer(design_vars) ## designs to be generated
# y <- abd_normalizer(ABD)
# Q <- ss_predicted_norm

## Invert the normalization
ss_predicted_norm <- predict(model, as.matrix(X)) 
ss_predicted <- Y_normalizer(ss_predicted_norm, invert = TRUE)

## Have eta include the average fiber VF
design_vars <- cbind(fiber_props, matrix_props, apply(fiber_vf, 1, mean))
eta_normalizer <- normalizer(design_vars)
abd_normalizer <- normalizer(ABD)
#layup_normalizer <- normalizer(layup_class)
#layup_orientation_normalizer <- normalizer(as.vector(unlist(lam_layups)))

## The three variables needed for OED
eta <- eta_normalizer(design_vars) ## designs to be generated
y <- abd_normalizer(ABD)
Q <- ss_predicted_norm
```

```{r}
library("rmi")

## Differential entropy estimator using KNN distances
H <- function(X, k = "estimate"){
#   if (is.null(dim(X)) && is.vector(X)){ X <- matrix(X) }
#   if (missing(k) || k == "estimate") { k <- ifelse(nrow(X) > 15, 15L, 2L) }
#   { d <- ncol(X); N <- nrow(X) }
#   thre <- 3 * (log(N)^(2/N))^(1/d)
#   knn <- apply(rmi::nearest_neighbors(X, k)$nn_dist, 1, max) # uses max norm
#   -digamma(k) + digamma(N) + d*log(2) + d*mean(log(knn[knn < thre]))
# }
# 
# # Returns the I(X; Y | Z)
# cMI <- function(X, Y, Z){
#   # browser()
#   h_xz <- H(cbind(X, Z))
#   h_yz <- H(cbind(Y, Z))
#   h_xyz <- H(cbind(X, Y, Z))
#   h_z <- H(Z)
#   return(h_xz + h_yz - h_xyz - h_z)
# }

truncate_between <- function(x, a = 0, b = 1){
  x[x < a] <- a
  x[x > b] <- b
  return(x)
}

## Expected utility function (to give to the approximate coordinate exchange algorithm)
makeUTILITY <- function(){
  counter <- 0L
  n_layups <- ncol(lam_layups)
  const_idx <- ncol(layup_one_hot)+1
  n_const <- (ncol(fiber_props)+ncol(matrix_props))
  layup_class_idx <- lapply(1:n_layups, function(li){ which(layup_class == li) })
  layup_cprop <- lapply(layup_class_idx, function(li){
    as.matrix(X[li, const_idx:(const_idx + n_const - 1)])
  })
  
  ## Single laminate
  # c_cp <- as.matrix(X[, 1:4])
  # c_abd <- as.matrix(ABD[c_layup_idx,])
  
  # k = 6, fiber E and V, matrix E and V, volume fraction, and laminate type
  util <- function(d, B, verbose = TRUE, ...){
    #browser()
    gen_fiber_p <- d[, 1:3]
    gen_matrix_p <- d[, 4:6]
    gen_fvf <- t(sapply(d[, 5], function(vf) rep(vf, nrow(lam_layups))))
    gen_layup <- truncate_between(d[, 6], 0 , 1)
    gen_layup_class <- round(layup_normalizer(gen_layup, invert = TRUE))
    gen_layout_hot <- to_categorical(y = gen_layup_class-1L, num_classes = length(layups))
    gen_layout_orientation <- t(layup_orientation_normalizer(lam_layups[, gen_layup_class]))
    
    ## Find the ABD matrix in the data set most similar to the generated constituent properties
    abd_idx <- sapply(1:nrow(d), function(i){
      c_class <- (gen_layup_class[[i]])
      layup_class_idx[[c_class]][closest_sample(d[i, ], layup_cprop[[c_class]]) + 1L]
    })
    gen_abd <- abd_normalizer(ABD[abd_idx,])
    
    ## Single laminate case
    # abd_idx <- sapply(1:nrow(d), function(i){ closest_sample(x = d[i, 1:4], constituent_samples = c_cp)+1L })
    # gen_abd <- abd_normalizer(c_abd[abd_idx,])
    # gen_abd[is.na(gen_abd)] <- 0
    
    X_test <- cbind(gen_layout_hot, 
                    gen_fiber_p, gen_matrix_p, 
                    gen_fvf, 
                    gen_layout_orientation, 
                    gen_abd)
      
    Q_test <- predict(model, as.matrix(X_test)) 
    cmi_res <- MaterialAnalysis::cMI(Q_test, Y = gen_abd, Z = d)
    if (verbose){
      cat(sprintf('\r %d', counter))
      counter <<- counter + 1L
    }
    return(cmi_res)
  }
}

# make_cMI <- function(X, Y){
#   counter <- 0L
#   util <- function(d, B, ...){
#     # browser()
#     # nearest_tests <- apply(d, 1, function(d_hat){ 
#     #   which.min(apply(orientati, 1, function(hat_eta) sum((hat_eta - d_hat)^2)))
#     # })
#     # X <- cbind(layup_one_hot, orientations_norm, apply(cbind(fiber_vf, ABD), 2, normalize))
#     # c_layup <- apply(d, 1, function(d_hat){
#     #   which.min(apply(lam_layups, 2, function(la) sum((la - d_hat)^2)))
#     # })
#     eta_idx <<- sapply(as.integer(d), function(cl) head(which(layup_class == cl), 1))
#     if (is.list(eta_idx)){ 
#       bad_idx <- which(sapply(eta_idx, is.na))
#       eta_idx[bad_idx] <- sample(1:length(layups), size=length(bad_idx), replace = TRUE)
#       eta_idx <- as.integer(unlist(eta_idx))
#       warning(sprintf("Detected %d NAs in the optimization", length(bad_idx)))
#     }
#     X_test <- cbind(layup_one_hot[eta_idx,], eta[eta_idx,], y[eta_idx,])
#     Q_test <- predict(model, as.matrix(X_test)) 
#     cmi_res <- cMI(Q_test, y[eta_idx,], Z = eta[eta_idx,])
#     cat(sprintf('\r %d', counter))
#     counter <<- counter + 1L
#     return(cmi_res)
#   }
#   return(util)
# }


## Design parameters 
## For this, we need at a minimum the constituentive properties (E and V for fiber and matrix), 
## the laminate type (integer from 1 to the number of laminate types), and 
## the (mean) volume fraction of the fiber. 
{ n <- 50; k <- 3+3+1+1 } # design dimensionality 
library("lhs")
res <- vector(mode = "list", length = 25L)
for (i in 1:25){
  start.d <- matrix(2 * randomLHS(n = n, k = k) - 1, nrow = n, ncol = k)
  util <- makeUTILITY()
  res[[i]] <- acebayes::ace(utility = util, start.d = start.d, 
                            deterministic = TRUE, 
                            lower = matrix(-1, nrow = n, ncol = k), upper = matrix(1, nrow = n, ncol = k), 
                            N1 = 20, N2 = 5, Q = 15)
  save(res, file = "~/res4.rdata")
}

load("~/res.rdata")

# res2 <- acebayes::ace(utility = util, start.d = start.d, deterministic = TRUE, lower = 1L, upper = 19L)
```

```{r}
sapply(1:4, function(i){
  optimal_design <- eta_normalizer(res[[1]]$phase1.d, invert = TRUE)
  optimal_design[, 6] <- round(optimal_design[, 6])
})

optimal_designs <- lapply(1:4, function(i){
   eta_normalizer(res[[i]]$phase1.d, invert = TRUE)
})

fiber_vf_od <- sapply(1:4, function(i){ eta_normalizer(res[[i]]$phase1.d, invert = TRUE)[, 5] })
layout(mat = matrix(c(1:4), nrow = 2, ncol = 2))
hist(fiber_vf_od[, 1])
hist(fiber_vf_od[, 2])
hist(fiber_vf_od[, 3])
hist(fiber_vf_od[, 4])

hist(design_vars[, 1])
hist(optimal_design[, 1])

## Fixing 
eta[, 1]

randomized_design <- 

hist(apply(fiber_vf, 1, mean))

colnames(optimal_design) <- c("E_f", "V_f", "E_m", "V_m", "VF", "Lam")


oed <- res[[1]]$phase1.d
colnames(oed) <- c("E_f", "V_f", "E_m", "V_m", "VF")
{ cond1 <- 1L; cond2 <- 3L }

genFixedConditions <- function(){
  fixed_conditions <- matrix(0, nrow = nrow(oed), ncol=ncol(oed))
  for (i in 1:ncol(oed)){
    if (!i %in% c(cond1, cond2)){ 
      fixed_conditions[, i] <- rnorm(nrow(oed), mean = mean(oed[, i]), sd = sd(oed[, i]))
    }
  }
  return(fixed_conditions)
}

c(-0.2, -1.0)

x_i <- y_i <- seq(-1, 1, by = 0.1)
f <- function(x, y){
  mean(sapply(1:50, function(i){scale_color_brewer(palette="Dark2")
    fixed_conditions <- genFixedConditions()
    fixed_conditions[, cond1] <- rnorm(n = n, mean = x, sd = 0.1/3)
    fixed_conditions[, cond2] <- rnorm(n = n, mean = y, sd = 0.1/3)
    util(fixed_conditions, B = NULL, verbose = FALSE)
  }))
}


indices <- expand.grid(x_i, y_i)
eig <- apply(indices, 1, function(idx) {
  cat(".")
  f(x = idx[1], y = idx[2])
})
eig_res <- matrix(0, nrow=length(x_i), ncol=length(y_i))
idx <- as.matrix(expand.grid(1:length(x_i), 1:length(x_i)))
eig_res[idx] <- eig

# tmp <- matrix(0, nrow = 21, ncol = 21)
# idx <- t(combn(1:length(x_i), 2))
# tmp[idx[, 1], idx[,2]]
# tmp[idx[, 1], idx[,2]] <- eig

wut <- as.matrix(as.dist(tmp[-21, -1]))
leig_res <- log(eig_res)
lattice::levelplot(log(eig_res), xlab = "E_f Information Gain", ylab = "E_m Information Gain", 
                   col.regions=heat.colors(20))
plot(density(design_vars[, cond1]))
plot(density(design_vars[, cond2]))

library("ggplot2")
idx_of_interest <- (40141-100):(40141+100)
eig_dt <- data.frame(Var1=idx[,1], Var2=idx[,2], value = eig[idx_of_interest])
ggplot(eig_dt, aes(x = Var1, y = Var2)) +
  geom_raster(aes(fill = value), interpolate=TRUE) +
  scale_fill_gradient2(low="blue", mid = "blue", high="red") +
  theme_classic()

plot(density(eta_normalizer(eta, invert = TRUE)[, cond1]))
plot(density(eta_normalizer(eta, invert = TRUE)[, cond2]))


plot(cbind(y[, 1], Q[, 1]))

range(Q[, 1])

y[,1]



oed <- eta_normalizer(res[[1]]$phase1.d, invert = TRUE)
plot(density(oed[, cond1]))
plot(density(oed[, cond2]))
```


```{r}
barplot(table(layups[as.integer(res$phase2.d)]), xaxt = "n")
text(cex=1, x=x-.25, y=-1.25, labs, xpd=TRUE, srt=45)

res1_tbl <- table(layups[as.integer(res$phase2.d)])
barplot(res1_tbl, las=2, cex.names = 0.6)
# text(cex=1, x=1:length(res2_tbl), y=-1.25, labels=names(res2_tbl), xpd=TRUE, srt=90)
layups[which(!layups %in% names(res1_tbl))]

res2_tbl <- table(layups[as.integer(res2$phase2.d)])
barplot(res2_tbl, las=2, cex.names = 0.6)
# text(cex=1, x=1:length(res2_tbl), y=-1.25, labels=names(res2_tbl), xpd=TRUE, srt=90)
layups[which(!layups %in% names(res2_tbl))]
```



Interpreting how close the predictions are
```{r}
plot(ss_props[, c(2, 1)], ylim = c(0, max(ss_props[, 1])))
# points(ss_predicted[, c(2, 1)])

col_pal <- rainbow(length(layups))
for (i in 1:length(layups)){
  c_layup_idx <- which(layup_class == i) # current layup indices
  c_layup_ss <- data.table::rbindlist(lapply(ss_result[c_layup_idx], as.data.frame), idcol = "id")
  
  # sub_lam_id <- 2
  # plot(c_layup_ss[id == sub_lam_id, .(V1, V2)], type = "l")
  # points(ss_props[c_layup_idx][, .(ult_tensile_strain, ult_tensile_stress)][sub_lam_id,], col = "red")
  
  plot(NULL, xlim=range(c_layup_ss$V1), ylim=range(c_layup_ss$V2), xlab="Strain", ylab="Stress", main=layups[i])
  sapply(unique(c_layup_ss$id), function(lid) {
    lines(c_layup_ss[id == lid, .(V1, V2)], col = adjustcolor("black", alpha.f = 0.45))
  })
  points(ss_props[c_layup_idx, 2:1], col = col_pal[i])
  points(ss_predicted[c_layup_idx, 2:1], col = "green")
}

for (i in 1:length(layups)){
  c_layup_idx <- which(layup_class == i) # current layup indices
  pred_res <- model %>% evaluate(as.matrix(X[c_layup_idx,]), as.matrix(Y[c_layup_idx,]))
  print(sprintf("%s mse: %f", layups[i], pred_res$mean_absolute_error))
}

model %>% evaluate(data_splits$train$X, data_splits$train$Y)
model %>% evaluate(data_splits$val$X, data_splits$val$Y)
model %>% evaluate(data_splits$test$X, data_splits$test$Y)

```



```{r}
x_pts <- cmdscale(dist(X[, 20:94]), k = 2)
cols <- rep("black", nrow(X))
cols[Y == 1] <- "red"
plot(x_pts, col = cols)

## TSNE visuals
idx <- sapply(unique(layup_class), function(i) sample(which(layup_class==i), size = 200))
x_tmp <- X_normalizer(X_tmp)[idx,]
x_pts_t <- tsne::tsne(dist(as.matrix(x_tmp)))
leg_cols <- sample(rainbow(length(layups)))
x_class <- layup_class[idx]

par(mar=c(5.1, 4.1, 4.1, 10.1), xpd=TRUE)
plot(x_pts_t, col = leg_cols[x_class], 
     xlab = "SNE_dim1", ylab = "SNE_dim2", main = "tSNE Embedding of 19 laminate types")
legend("topright", xpd=TRUE, legend=paste0(1:length(layups), ": ", layups), col=leg_cols, lty=1, cex=0.65, 
       inset=c(-0.33,0))

centroids <- t(sapply(1:length(layups), function(i) apply(x_pts_t[x_class == i,], 2, mean)))
text(centroids, labels = 1:length(layups), 
     pos = 3, 
     col = adjustcolor(leg_cols, red.f = 0.5, green.f = 0.5, blue.f = 0.5),
     cex=1.5)

```

## Creating the Optimal Experimental Design

The vast majority of OED in field is performed with very low-dimensional design spaces, often with well-supported linear models which assume some (additive) form of Gaussian error. Although such models are quite flexible and potentially powerful, they often require domain-specific knowledge to create and refine effectively. 

A Bayesian OED is founded by maximizing the expectation of some give utility function $u$ over the space of all possible designs. 

# ```{r}
# # volume of d-dimensional ball w/ radius R 
# Vball <- function(d, R=1){  pi^(d/2)/gamma(d/2 + 1) * R^d } 
# 
# H <- function(X, k="estimate"){
#   if (is.null(dim(X)) && is.vector(X)){ X <- matrix(X) }
#   if (missing(k) || k == "estimate") { k <- ceiling(ncol(X)*log(nrow(X))) }
#   { d <- ncol(X); N <- nrow(X) }
#   eps <- apply(FNN::knn.dist(X, k = k), 1, max)
#   -digamma(k) + digamma(N) + log(Vball(d)) + (d/N)*sum(log(2*eps))
# }
# 
# 
# 
# 
# 
# ## Mutual information
# # I <- function(X, Y, ...){
# #   if (is.null(dim(X)) && is.vector(X)){ X <- matrix(X) }
# #   if (is.null(dim(Y)) && is.vector(Y)){ Y <- matrix(Y) }
# #   N <- nrow(X)
# #   { d_x <- ncol(X); d_y <- ncol(Y) }
# #   k <- ceiling((d_x + d_y)*log(N))
# #   { X_dist <- dist(X); Y_dist <- dist(Y) } 
# #   Z_norm <- as.dist(pmax(X_dist*2, Y_dist*2))
# #   z_knn <- dbscan::kNN(Z_norm, k = k, sort = FALSE)
# #   eps <- apply(z_knn$dist, 1, max)
# #   { X_dm <- as.matrix(X_dist); Y_dm <- as.matrix(Y_dist) }
# #   n_x <- sapply(1:N, function(i){ sum(X_dm[i,] < eps[i]/2) - 1L })
# #   n_y <- sapply(1:N, function(i){ sum(Y_dm[i,] < eps[i]/2) - 1L })
# #   digamma(k) + mean(digamma(n_x + 1L) + digamma(n_y + 1L)) + digamma(N)
# # }
# I <- function(X, Y, k){ ## Kraskovs knn-based mutual information
#   if (is.null(dim(X)) && is.vector(X)){ X <- matrix(X) }
#   if (is.null(dim(Y)) && is.vector(Y)){ Y <- matrix(Y) }
#   N <- nrow(X)
#   { dx <- ncol(X); dy <- ncol(Y) }  	
#   max_knn <- dbscan::kNN(dist(cbind(X, Y), method = "maximum"), k = k, sort = FALSE)
#   max_knn_dist <- apply(max_knn$dist, 1, max) - sqrt(.Machine$double.eps) # nearest_neighbors(X, 3)
#   X_dm <- as.matrix(dist(X, method = "maximum"))
#   Y_dm <- as.matrix(dist(Y, method = "maximum"))
#   n_x <- sapply(1:N, function(i){ sum(X_dm[i,] < max_knn_dist[i]) - 1L })
#   n_y <- sapply(1:N, function(i){ sum(Y_dm[i,] < max_knn_dist[i]) - 1L })
#   ans_x <- digamma(N) + dx*log(2) + sum(-digamma(n_x)/N+dx*log(max_knn_dist)/N)
#   ans_y <- digamma(N) + dy*log(2) + sum(-digamma(n_y)/N+dy*log(max_knn_dist)/N)
#   ans_xy <- -digamma(k) + digamma(N) + (dx+dy)*log(2) + sum((dx+dy)*log(max_knn_dist)/N)
#   return(ans_x+ans_y-ans_xy)
# }
# 
# ## Returns a closured way of pseudo-efficiently calculating the conditional mutual information
# ## I(X, Y | Z) given new values of Z, assuming X and Y are fixed. 
# cMI <- function(X, Y, Z, k = "suggest"){
#   if (is.null(dim(X)) && is.vector(X)){ X <- matrix(X) }
#   if (is.null(dim(Y)) && is.vector(Y)){ Y <- matrix(Y) }
#   if (is.null(dim(Z)) && is.vector(Z)){ Z <- matrix(Z) }
#   N <- nrow(X)
#   { d_x <- ncol(X); d_y <- ncol(Y); d_z <- ncol(Z) }
#   if (missing(k) || k == "suggest") k <- ceiling(log(N)*d_x)
#   
#   ## Needed 
#   mi_xy <- rmi::knn_mi(cbind(X, Y), splits = c(d_x, d_y), options = list(method = "KSG1", k = k))
#   
#   ## Variables needed to be stored in the closure environment
#   X_dm <- as.matrix(dist(X, method = "maximum"))
#   Y_dm <- as.matrix(dist(Y, method = "maximum"))
#   max_knn <- dbscan::kNN(dist(cbind(X, Y), method = "maximum"), k = k, sort = FALSE)
#   max_knn_dist <- apply(max_knn$dist, 1, max) - sqrt(.Machine$double.eps) # nearest_neighbors(X, 3)
# 
#   n_x <- sapply(1:N, function(i){ sum(X_dm[i,] < max_knn_dist[i]) - 1L })
#   n_y <- sapply(1:N, function(i){ sum(Y_dm[i,] < max_knn_dist[i]) - 1L })
#   ans_x <- digamma(N) + dx*log(2) + sum(-digamma(n_x)/N+dx*log(max_knn_dist)/N)
#   ans_y <- digamma(N) + dy*log(2) + sum(-digamma(n_y)/N+dy*log(max_knn_dist)/N)
#   ans_xy <- -digamma(k) + digamma(N) + (dx+dy)*log(2) + sum((dx+dy)*log(max_knn_dist)/N)
#   
#   cmi <- function(Z){
#     
#   }
# }
# 
# I(X, Y, k = 5)
# rmi::knn_mi(cbind(X, Y), splits = c(3, 3), options = list(method = "KSG1", k = 5))
# ```
# 
# ```{r}
# library("rmi")
# 
# # Calculates the conditional mutual information, i.e. I(X; Y | Z)
# cMI <- function(X, Y, Z, estimator=list(method = "KSG", k = 5)){
#   if (is.null(dim(X)) && is.vector(X)){ X <- matrix(X) }
#   if (is.null(dim(Y)) && is.vector(Y)){ Y <- matrix(Y) }
#   if (is.null(dim(Z)) && is.vector(Z)){ Z <- matrix(Z) }
#   { d_x <- ncol(X); d_y <- ncol(Y); d_z <- ncol(Z) }
#   mmi <- rmi::knn_mi(cbind(X, Y, Z), rep(1, d_x + d_y + d_z), options=estimator) # multivariate mutual information
#   mi_xz <- knn_mi(cbind(X, Z), c(d_x, d_z), options=estimator)
#   mi_xy <- knn_mi(cbind(X, Y), c(d_x, d_y), options=estimator)
#   H_x <- rmi::lnn_entropy(X)
#   H_z <- rmi::lnn_entropy(Z)
#   return(mmi - (mi_xz + mi_xy) + H_x - H_z)
# } 
# 
# 
# ## testing MI estimator
# XY <- MASS::mvrnorm(n = 100, mu = c(0, 0), Sigma = matrix(c(1, 0, 0, 1), ncol = 2))
# -0.5 * log(1 - 0^2)
# ```
# 
# 
# 
# Building a model to classify between the different layups 
# 
# ```{r}
# source('~/grc-materials-analysis/MaterialAnalysis/R/NN.R')
# 
# library("keras")
# library("tensorflow")
# 
# ABD <- do.call(rbind, lapply(res_out, function(rout) unlist(rout[["section II"]]$ABD_matrix)))
# fiber_vf <- do.call(rbind, lapply(res_mac, function(lam) lam$VF))
# 
# 
# ## Inputs and Outputs
# X <- apply(cbind(ABD, fiber_vf), 2, normalize) ## ensure any(is.na(X)) == FALSE
# 
# lam_layups <- sapply(1:length(layups), function(i) eval(as.symbol(sprintf("lam%d", i))))
# Y <- sapply(res_mac, function(lam_conf){
#   which(apply(lam_layups, 2, function(lu) all(lu == lam_conf$ANG)))
# })
# 
# ## Configure the inputs 
# X <- data.table::data.table(X)
# data_splits <- partition(X = X, Y = Y, input_shape = c(51), 
#                          splits = c(train = 0.80, validate = 0.10, test = 0.10))
# 
# ## The decision layers
# decision_layers <- list(
#   "dense" = list(units = 25, activation = 'tanh'),
#   "dropout" = list(rate = 0.35),
#   "dense" = list(units = 25, activation = 'tanh'),
#   "dropout" = list(rate = 0.35),
#   "dense" = list(units = 25, activation = 'tanh'),
#   "dense" = list(units = 11, activation = 'softmax')
# )
# 
# ## Generate the layer tensors
# keras::k_clear_session()
# main_input <- layer_input(shape = c(51), name = 'main_input')
# 
# ## Connect the inputs
# dense_net <- MLP(layers = decision_layers)
# predictions <- main_input %>% dense_net
# 
# ## Make the model
# model <- keras_model(inputs = main_input, outputs = predictions)
# 
# ## Compile the model with the selected loss and optimizer
# model %>% compile(
#   loss = 'categorical_crossentropy',
#   # optimizer = optimizer_rmsprop(),
#   optimizer = optimizer_nadam(),
#   metrics = c('accuracy')
# )
# 
# ## Train the model
# history <- model %>% fit(
#   x = data_splits$train$X,
#   y = data_splits$train$Y, 
#   epochs = 250, batch_size = 30, 
#   # validation_split = 0.20
#   validation_data = list(data_splits$val$X, data_splits$val$Y)
# )
# 
# ## See how it performs on the different partitions of the data
# model %>% evaluate(data_splits$train$X, data_splits$train$Y)
# model %>% evaluate(data_splits$val$X, data_splits$val$Y)
# model %>% evaluate(data_splits$test$X, data_splits$test$Y)
# 
# ```



<!-- Constructing a cover:  -->

<!-- By linear scan:  -->
<!--   For each dimensions $d$,  -->
<!--   1) sort $O(n log n)$ -->
<!--   2)  -->

<!-- ```{r} -->
<!-- x <- runif(5000) -->
<!-- begin_pts <- seq(0, 0.95, length.out = 10) -->
<!-- end_pts <- seq(0.05, 1, length.out = 10) -->
<!-- findInterval(x, vec = begin_pts) -->
<!-- microbenchmark::microbenchmark( findInterval(x, vec = endpoints) ) -->
<!-- ``` -->
  
  