#' contains the MAC/GMC executable.
#' @export
runMAC <- function(src_dir, dest_dir, base_fn, user, pass, host = "cryoterm1", mac="mac4z-3_9"){
src_dir <- normalizePath(path = src_dir)
src_files <- list.files(src_dir)
src_mac_files <- src_files[grep(src_files, pattern = "*.mac")]
## Generate the bash file
n_files <- length(src_mac_files)
cpu_avail <- c(1, 5, 10, 15, 20, 25)
res <- n_files/c(1, 5, 10, 15, 20, 25)
n_cpus <- cpu_avail[max(which(res == floor(res)))]
offset <- 0L
if (n_cpus == n_files){
mac_lines <- sprintf("./%s %s_%s", mac, base_fn, sprintf("$((SLURM_ARRAY_TASK_ID))"))
} else {
indices <- cumsum(rep(n_cpus, (n_files/n_cpus))) - n_cpus
mac_lines <- sprintf("./%s %s_%s", mac, base_fn, sprintf("$((SLURM_ARRAY_TASK_ID + %d))", indices))
}
if (length(mac_lines) == 0){ stop("Stopping; no mac files found.") }
sbatch_lines <- c(
"#!/bin/bash",
"#SBATCH -p acp",
"# Submit an array job with tasks 1-25, allow 25 to run simultaneously:",
"#SBATCH --array=1-25%25",
"#SBATCH --nodelist=acp3",
"#SBATCH -o slurm-job-%j.output",
"#SBATCH -e slurm-job-%j.error",
"#SBATCH --mem_bind=local",
"#SBATCH --export=ALL",
"echo $SLURM_ARRAY_TASK_ID > slurm_task_id_save",
"ulimit -s unlimited",
"module load intel",
mac_lines,
"module rm intel",
"false")
readr::write_lines(x = sbatch_lines, path = file.path(src_dir, "sbatch_script.sh"))
sec_check <- paste0("sshpass -p '", eval(pass), "'")
## Remove any current MAC or OUT files
rm_files <- "find %s -maxdepth 1 -type f -name '%s' -delete"
dquote <- function(x){ paste0("\"", x, "\"") }
squote <- function(x){ paste0("'", x, "'") }
system(sprintf("%s ssh %s@%s %s", sec_check, user, host, dquote(sprintf(rm_files, dest_dir, "*.mac"))), ignore.stdout = TRUE, ignore.stderr = TRUE)
system(sprintf("%s ssh %s@%s %s", sec_check, user, host, dquote(sprintf(rm_files, dest_dir, "*.out"))), ignore.stdout = TRUE, ignore.stderr = TRUE)
system(sprintf("%s ssh %s@%s %s", sec_check, user, host, dquote(sprintf(rm_files, dest_dir, "*.data"))), ignore.stdout = TRUE, ignore.stderr = TRUE)
system(sprintf("%s ssh %s@%s %s", sec_check, user, host, dquote(sprintf(rm_files, dest_dir, "slurm*"))), ignore.stdout = TRUE, ignore.stderr = TRUE)
## Copy all the files over
message("Copying files over...")
scp_command <- paste0(sec_check,
" scp ",
file.path(normalizePath(src_dir), "*"),
sprintf(" %s@%s:%s", user, host, dest_dir))
system(scp_command, ignore.stdout = TRUE, ignore.stderr = TRUE) ## copies the batch file + the MAC files
## Run the sbatch script
message("Running MAC/GMC...")
system(sprintf("%s ssh %s@%s %s", sec_check, user, host, squote(sprintf("cd %s && sbatch sbatch_script.sh", dest_dir))), ignore.stdout = TRUE, ignore.stderr = TRUE)
## Periodically check the status
status_check_cmd <- sprintf("%s ssh %s@%s 'squeue'", sec_check, user, host)
status_check_cmd2 <- sprintf("%s ssh %s@%s 'ls %s'", sec_check, user, host, file.path(dest_dir, "*.out"))
pb <- txtProgressBar(min = 0, max = length(src_mac_files), style = 3)
status_check <- function(){
status_lines <- system(status_check_cmd, intern = TRUE, ignore.stdout = FALSE, ignore.stderr = TRUE)
out_files <- system(status_check_cmd2, intern = TRUE, ignore.stdout = FALSE, ignore.stderr = TRUE)
Sys.sleep(0.5)
status <- do.call(rbind, strsplit(status_lines, split = "\\s+"))
setTxtProgressBar(pb, value = length(out_files))
(!user %in% status[, 5]) && length(out_files) == length(src_mac_files)
}
## Wait for MAC to finish
while(!status_check()){ Sys.sleep(time = 2) }
close(pb)
## Retrieve the out files
retrieve_data_base <- sprintf("scp %s@%s:%s %s", user, host, file.path(dest_dir, "*.data"), src_dir)
retrieve_out_base <- sprintf("scp %s@%s:%s %s", user, host, file.path(dest_dir, "*.out"), src_dir)
message(paste0("Retrieving data files with: ", retrieve_data_base))
system(sprintf("%s %s", sec_check, retrieve_data_base), ignore.stdout = TRUE, ignore.stderr = TRUE)
message(paste0("Retrieving data files with: ", retrieve_data_base))
system(sprintf("%s %s", sec_check, retrieve_out_base), ignore.stdout = TRUE, ignore.stderr = TRUE)
# system("sshpass -p 'Char_277214vin++' scp ~/GENERATED_MAC/sbatch_script.sh mpiekenb@cryoterm1:~/testing/ ")
}
shiny::runApp('CopyofMattsMachine-Aug2018/MaterialAnalysis/inst/material_dashboard')
#' dashboard
#' @description Runs the Material Analysis dashboard.
#' @param X material dataset (optional)
#' @import data.table shiny
#' @export
dashboard <- function(X = NULL, dev_mode = TRUE){
if (!missing(X)){
dimensions <- colnames(X)
X <- parseData(X, dim = dimensions, all_numeric = FALSE, remove.NA = FALSE, scale = FALSE)
X <- X[, lapply(.SD, function(x_i) if (is.character(x_i) || is.factor(x_i)) as.factor(x_i) else as.numeric(x_i))]
}
## If developing the app, set to true to work from the files directly. FALSE will only use the files loaded with
## the MaterialsAnalysis package.
if (dev_mode){
app_dir <- file.path("/home", "mpiekenb", "grc-materials-analysis", "MaterialAnalysis", "inst", "material_dashboard")
message(sprintf("Development mode\nRunning app with files at: %s", app_dir))
} else {
app_dir <- system.file(package = "MaterialAnalysis")
}
shiny::runApp(appDir = app_dir)
# dashboard_file <- system.file(file.path('material_dashboard', 'ui_components', 'psp_dashboard_ui.R'), package = "MaterialAnalysis")
# server_file <- system.file(file.path('material_dashboard', 'reactive', 'psp_dashboard_server.R'), package = "MaterialAnalysis")
# source(dashboard_file, local = TRUE)
# source(server_file, local = TRUE)
# shiny::runApp(shinyApp( ui = ui, server = server ))
}
runApp('CopyofMattsMachine-Aug2018/MaterialAnalysis/inst/material_dashboard')
load("~/CopyofMattsMachine-Aug2018/MaterialAnalysis/eff_properties.rdata")
asdf
http://127.0.0.1:8888
library(ISLR)
set.seed(1)
glm.fit=glm(mpg~horsepower,data=Auto)
coef(glm.fit)
lm.fit=lm(mpg~horsepower,data=Auto)
coef(lm.fit)
library(boot)
glm.fit=glm(mpg~horsepower,data=Auto)
cv.err$delta
cv.error=rep(0,5)
for (i in 1:5){
glm.fit=glm(mpg~poly(horsepower,i),data=Auto)
cv.error[i]=cv.glm(Auto,glm.fit)$delta[1]
}
cv.error
library(ISLR)
set.seed(1)
train=sample(392,196)
lm.fit=lm(mpg~horsepower,data=Auto,subset=train)
attach(Auto)
mean((mpg-predict(lm.fit,Auto))[-train]^2)
lm.fit2=lm(mpg~poly(horsepower,2),data=Auto,subset=train)
mean((mpg-predict(lm.fit2,Auto))[-train]^2)
lm.fit3=lm(mpg~poly(horsepower,3),data=Auto,subset=train)
mean((mpg-predict(lm.fit3,Auto))[-train]^2)
set.seed(2)
train=sample(392,196)
lm.fit=lm(mpg~horsepower,subset=train)
mean((mpg-predict(lm.fit,Auto))[-train]^2)
lm.fit2=lm(mpg~poly(horsepower,2),data=Auto,subset=train)
mean((mpg-predict(lm.fit2,Auto))[-train]^2)
lm.fit3=lm(mpg~poly(horsepower,3),data=Auto,subset=train)
mean((mpg-predict(lm.fit3,Auto))[-train]^2)
glm.fit=glm(mpg~horsepower,data=Auto)
coef(glm.fit)
lm.fit=lm(mpg~horsepower,data=Auto)
coef(lm.fit)
library(boot)
glm.fit=glm(mpg~horsepower,data=Auto)
cv.err=cv.glm(Auto,glm.fit)
cv.err$delta
cv.error=rep(0,5)
for (i in 1:5){
glm.fit=glm(mpg~poly(horsepower,i),data=Auto)
cv.error[i]=cv.glm(Auto,glm.fit)$delta[1]
}
cv.error
set.seed(17)
cv.error.10=rep(0,10)
for (i in 1:10){
glm.fit=glm(mpg~poly(horsepower,i),data=Auto)
cv.error.10[i]=cv.glm(Auto,glm.fit,K=10)$delta[1]
}
cv.error.10
alpha.fn=function(data,index){
X=data$X[index]
Y=data$Y[index]
return((var(Y)-cov(X,Y))/(var(X)+var(Y)-2*cov(X,Y)))
}
alpha.fn(Portfolio,1:100)
set.seed(1)
alpha.fn(Portfolio,sample(100,100,replace=T))
boot(Portfolio,alpha.fn,R=1000)
plot(degree, cv.error, type="b")
# 10-fold CV
cv.error10=rep(0,5)
cv.error.10=rep(0,10)
for (i in 1:10){
glm.fit=glm(mpg~poly(horsepower,i),data=Auto)
cv.error.10[i]=cv.glm(Auto,glm.fit,K=10)$delta[1]
}
cv.error.10
degree = rep(0,5)
lines(degree,cv.error.10, type='b')
degree = rep(0,10)
lines(degree,cv.error.10, type='b')
plot(degree, cv.error, type="b")
plot(degree, cv.error.10, type="b")
lines(degree,cv.error.10, type='b')
for (i in 1:10){
glm.fit=glm(mpg~poly(horsepower,i),data=Auto)
cv.error.10[i]=cv.glm(Auto,glm.fit,K=10)$delta[1]
}
cv.error.10
degree = rep(0,10)
lines(degree,cv.error.10, type='b')
degree = 1:10
lines(degree,cv.error.10, type='b')
cv.error.10=rep(0,10)
for (i in 1:10){
glm.fit=glm(mpg~poly(horsepower,i),data=Auto)
cv.error.10[i]=cv.glm(Auto,glm.fit,K=10)$delta[1]
}
cv.error.10
degree = 1:10
plot(degree, cv.error.10, type="b")
lines(degree,cv.error.10, type='b')
(vy-cxy)/(vx+vy-2*cxy)
# Bootstrap 2
alpha = function(x,y){
vx=var(x)
vy=var(y)
cxy=cov(x,y)
(vy-cxy)/(vx+vy-2*cxy)
}
alpha(Portfolio$X,Portfolio$Y)
# What is the standard error of alpha?
alpha.fn=function(data,index){
with(data[index,],alpha(X,Y))
}
alpha.fn(Portfolio,1:100)
set seed(1)
set.seed(1)
alpha.fn(Portfolio,sample(1:100,100,replace=TRUE))
boot.out=boot(Portfolio,alpha.fn,R=100)
boot.out
plot(boot.out)
library(ISLR)
fix(Hitters)
names(Hitters)
knitr::opts_chunk$set(echo = TRUE)
summary(cars)
library(ISLR)
summary(Hitters)
Hitters=na.omit(Hitters)
with(Hitters,sum(is.na(Salary)))
knitr::opts_chunk$set(echo = TRUE)
summary(cars)
plot(pressure)
library(ISLR)
summary(Hitters)
parseElasticProperties <- function(file_path){
data_file <- readr::read_lines(file_path)
x <- paste(data_file, collapse = "\n")
numeric_regex <- "([-+]?[0-9]*\\.?[0-9]+(?:[eED][-+]?[0-9]+)?)"
ws_numeric <- paste0("\\s*", numeric_regex, "\\s*")
el_prop <- paste0("\\s+\\d+\\)\\s*(.*)\\s*=\\s*", numeric_regex)
# el_prop_regex <- paste0("Elastic Properties:", paste0(rep(el_prop, 7), collapse = ""))
res <- getMatches(x, el_prop)
elastic_props_names <- sapply(res, function(rgx_matches) trimws(rgx_matches[[1]]))
elastic_props_values <- as.numeric(gsub(pattern = "D", replacement = "E",
sapply(res, function(rgx_matches) trimws(rgx_matches[[2]]))))
n_materials <- length(elastic_props_names)/7
for (i in 1:n_materials){
idx <- (i - 1)*7 + 1
elastic_props_names[(idx):(idx+6)] <- sprintf("M%d: %s", i, elastic_props_names[(idx):(idx+6)])
}
return(list(el_names = elastic_props_names, el_values = elastic_props_values))
}
library("MaterialAnalysis")
## Collect the files to parse
out_files <- grep(pattern = ".*\\.out", list.files(path = "~/Desktop/Random_Laminate/"), value = TRUE)
mac_files <- grep(pattern = ".*\\.MAC", list.files(path = "~/Desktop/Random_Laminate/"), value = TRUE)
data_files <- grep(pattern = ".*\\.data", list.files(path = "~/Desktop/Random_Laminate/"), value = TRUE)
el_res <- pbapply::pblapply(out_files, function(out_file){
parseElasticProperties(file.path("~/Desktop", "Random_Laminate", out_file))
})
el_props <- do.call(rbind, lapply(el_res, function(el){ el$el_values }))
colnames(el_props) <- el_res[[1]]$el_names
## Location of mac output files
mac_folder <- "~/GENERATED_MAC/"
## Get a list of mac files
res_files <- list.files(mac_folder)
## Get MAC input properties
res_mac <- pbapply::pblapply(res_files[grep(res_files, pattern = "*.mac")], function(mac_file){
parseMAC(file_path = normalizePath(file.path("~/GENERATED_MAC/", mac_file)))
})
library(reticulate)
use_python("C:/Users/jstuckne/AppData/Local/Continuum/miniconda3/envs/r-reticulate/python.exe", required=TRUE)
# library(reticulate)
# use_python("C:/Users/jstuckne/AppData/Local/Continuum/miniconda3/envs/r-reticulate/python.exe", required=TRUE)
# library(reticulate)
# use_python("C:/Users/jstuckne/AppData/Local/Continuum/miniconda3/envs/r-reticulate/python.exe", required=TRUE)
# library(reticulate)
# use_python("C:/Users/jstuckne/AppData/Local/Continuum/miniconda3/envs/r-reticulate/python.exe", required=TRUE)
# library(reticulate)
# use_python("C:/Users/jstuckne/AppData/Local/Continuum/miniconda3/envs/r-reticulate/python.exe", required=TRUE)
library(reticulate)
library(reticulate)
asdf
library(reticulate)
use_python("C:/Users/jstuckne/AppData/Local/Continuum/miniconda3/envs/r-reticulate/python.exe", required=TRUE)
x = 1
require(MaterialAnalysis);
## Parses each row of a named data.frame into a tag list
parse_df <- function(df, pre_space = "", collapse=FALSE){
## Parse row by row
res <- vector("list", length=nrow(df))
for (i in seq(nrow(df))){
res[[i]] <- paste0(pre_space, parse_row(df[i,,drop=FALSE], colnames(df)))
}
## Unless collapse is requested, return a list of the parsed rows. Otherwise collapse w/ newline.
if (is.logical(collapse) && !collapse){
return(res)
} else {
return(paste0(unlist(res), collapse="\n"))
}
}
## Converts a vector with names into a tag=value space-delimited string
parse_row <- function(x, x_names = names(x), collapse = " "){
paste0(as.vector(sapply(x_names, function(h) {
paste0(h, "=", switch(readr::guess_parser(x[[h]]),
"integer"=paste0(as.integer(x[[h]]), collapse = ", "),
"double"=paste0(sprintf("%.5f", x[[h]]), collapse = ", "),
x[[h]]))
})), collapse = collapse)
}
## Layups to test
layups <- c("[0]_24",
"[90]_24",
"[0/90]_12",
"[0/90]_6S",
"[15, -15]_12",
"[30, -30]_12",
"[45, -45]_12",
"[60, -60]_12",
"[75, -75]_12",
"[15, -15]_6S",
"[30, -30]_6S",
"[45, -45]_6S",
"[60, -60]_6S",
"[75, -75]_6S",
"[0, -45, 45, 90]_3S",
"[0, -60, 60]_4S",
"[0, 90, -30, 30, -60, 60]_2S",
"[0, 45, 90, -45]_6",
"[45, -45, 90, 0]_3S")
for (li in 1:length(layups)){ assign(paste0("lam", li), value = makeLayup(layups[[li]])) }
lam_layups <- sapply(1:length(layups), function(i) eval(as.symbol(sprintf("lam%d", i))))
## Location of mac output files
mac_folder <- "~/GENERATED_MAC/"
## Get a list of mac files
res_files <- list.files(mac_folder)
## Get MAC input properties
res_mac <- pbapply::pblapply(res_files[grep(res_files, pattern = "*.mac")], function(mac_file){
parseMAC(file_path = normalizePath(file.path("~/GENERATED_MAC/", mac_file)))
})
## Get MAC input properties
res_mac <- pbapply::pblapply(res_files[grep(res_files, pattern = "*.mac")], function(mac_file){
parseMAC(file_path = normalizePath(file.path("~/GENERATED_MAC/", mac_file)))
})
View(res_mac)
## Get the resulting outputs from GMC
res_out <- pbapply::pblapply(res_files[grep(res_files, pattern = "*.out")], function(out_file){
parseOUT(file_path = normalizePath(file.path("~/GENERATED_MAC", out_file)), sections = 2L)
})
## load the inputs
load("~/experiment1.rdata")
## load the inputs
load("~/experiment1.rdata")
res_mac <- experiment1[[1]]
res_out <- experiment1[[2]]
res_data <- experiment1[[3]]
res_data <- experiment1[[3]]
res_ss_props <- experiment1[[4]]
## Normalize closure
normalizer <- function(x, a = -1, b = 1, rng=NULL){
# if (!is.null(opts) list("ab_range"=list(a=0, b=1), =list(center=TRUE, scale=TRUE))
if (is.vector(x)){
{ min_x <- min(x); max_x <- max(x) }
normalize <- function(x, invert=FALSE){
if (missing(invert) || !invert){ return(((b - a)*(x - min_x)) / (max_x - min_x) + a ) }
else { return(((x - a)/(b - a))*(max_x - min_x) + min_x) }
}
return(normalize)
}
else if (is.matrix(x)){
d_x <- ncol(x)
if (missing(rng) || is.null(rng)){
col_idx <- as.list(seq(ncol(x)))
x_rng <- apply(x, 2, range)
} else {
if (sum(rng) != d_x) { stop("'rng' should sum to the number fo columns in x.")}
start_idx <- cumsum(rng) - rng + 1
col_idx <- lapply(1:length(rng), function(i) start_idx[i]:(start_idx[i]+rng[i]-1L))
x_rng <- sapply(1:length(col_idx), function(i) { range(as.vector(unlist(x[, col_idx[[i]]]))) })
}
normalize <- function(x, invert=FALSE){
if (missing(invert) || !invert){
x_norm <- matrix(0, ncol = ncol(x), nrow = nrow(x))
for (i in 1:length(col_idx)){
idx <- col_idx[[i]]
x_tmp <- as.vector(unlist(x[, idx]))
x_norm[, idx] <- ((b - a)*(x_tmp - x_rng[1, i])) / (x_rng[2, i] - x_rng[1, i]) + a
}
return(x_norm)
}
else {
x_unnorm <- matrix(0, ncol = ncol(x), nrow = nrow(x))
for (i in 1:length(col_idx)){
idx <- col_idx[[i]]
x_tmp <- as.vector(unlist(x[, idx]))
x_unnorm[, idx] <- (((x_tmp - a)/(b - a))*(x_rng[2, i] - x_rng[1, i]) + x_rng[1, i])
}
return(x_unnorm)
}
}
}
return(normalize)
}
## Inputs (constituent properties, fiber volume fractions [per ply], layup, and ABD components)
fiber_props <-  do.call(rbind, lapply(res_mac, function(mac) mac$fiber_constituent_properties[c(1, 3, 8)]))
matrix_props <-  do.call(rbind, lapply(res_mac, function(mac) mac$matrix_constituent_properties[c(1, 3, 8)]))
fiber_vf <- do.call(rbind, lapply(res_mac, function(mac) mac$laminate_properties$VF))
orientations <- do.call(rbind, lapply(res_mac, function(mac) mac$laminate_properties$ANG))
ABD <- do.call(rbind, lapply(res_out, function(rout) unlist(rout[["section II"]]$ABD_matrix)))
## Separate out the laminates by the layup
layup_class <- sapply(res_mac, function(mac){
which.min(apply(lam_layups, 2, function(lu) sum((lu - mac$laminate_properties$ANG)^2)))
})
## Separate out the laminates by the layup
layup_class <- sapply(res_mac, function(mac){
which.min(apply(lam_layups, 2, function(lu) sum((lu - mac$laminate_properties$ANG)^2)))
})
layup_one_hot <- to_categorical(y = layup_class - 1L, num_classes = length(layups))
use_python()
python
## Load components needed to build a neural net
source('~/Code/MaterialAnalysis/MaterialAnalysis/R/NN.R')
layup_one_hot <- to_categorical(y = layup_class - 1L, num_classes = length(layups))
layup_one_hot <- to_categorical(y = layup_class - 1L, num_classes = length(layups))
# c_layup <- 1:19
# c_layup_idx <- which(layup_class == c_layup)
# X_tmp <- cbind(fiber_props[c_layup_idx,],
#                matrix_props[c_layup_idx,],
#                fiber_vf[c_layup_idx,],
#                # orientations[c_layup_idx, ],
#                ABD[c_layup_idx,])
# X_normalizer <- normalizer(X_tmp)
# X <- X_normalizer(X_tmp)
# X[is.na(X)] <- 0
X_tmp <- cbind(fiber_props, matrix_props, fiber_vf, orientations, ABD)
X_normalizer <- normalizer(X_tmp,
rng = c(rep(1, ncol(fiber_props) + ncol(matrix_props)),
ncol(fiber_vf), ncol(orientations), ncol(ABD)))
X <- cbind(layup_one_hot, X_normalizer(X_tmp)) ## ensure any(is.na(X)) == FALSE
## Outputs (properties along the stress strain curve)
# c_ss_props <- as.matrix(res_ss_props[c_layup_idx,])
c_ss_props <- as.matrix(res_ss_props)
Y_normalizer <- normalizer(c_ss_props)
Y <- Y_normalizer(c_ss_props)
## Configure the inputs
X <- data.table::data.table(X)
data_splits <- partition(X = X, Y = Y, input_shape = c(ncol(X)),
splits = c(train = 0.70, validate = 0.20, test = 0.10))
## The model layers
decision_layers <- list(
"dense" = list(units = 25, activation = 'tanh'),
"dropout" = list(rate = 0.35),
"dense" = list(units = 45, activation = 'tanh'),
"dropout" = list(rate = 0.35),
"dense" = list(units = 45, activation = 'tanh'),
"dropout" = list(rate = 0.35),
"dense" = list(units = 25, activation = 'tanh'),
"dropout" = list(rate = 0.35),
"dense" = list(units = ncol(Y), activation = 'linear')
)
## Generate the layer tensors
keras::k_clear_session()
main_input <- layer_input(shape = c(ncol(X)), name = 'main_input')
## Connect the inputs
dense_net <- MLP(layers = decision_layers)
predictions <- main_input %>% dense_net
## Make the model
model <- keras_model(inputs = main_input, outputs = predictions)
## Compile the model with the selected loss and optimizer
model %>% compile(
loss = 'mean_squared_error',
# optimizer = optimizer_rmsprop(),
optimizer = optimizer_nadam(),
metrics = c('mae')
)
## Train the model
history <- model %>% fit(
x = data_splits$train$X,
y = data_splits$train$Y,
epochs = 250, batch_size = 30,
# validation_split = 0.20
validation_data = list(data_splits$val$X, data_splits$val$Y)
)
View(X)
View(Y)
View(X)
View(X_tmp)
## Invert the normalization
ss_predicted_norm <- predict(model, as.matrix(X))
ss_predicted <- Y_normalizer(ss_predicted_norm, invert = TRUE)
## Have eta include the average fiber VF
design_vars <- cbind(fiber_props, matrix_props, apply(fiber_vf, 1, mean))
eta_normalizer <- normalizer(design_vars)
abd_normalizer <- normalizer(ABD)
## The three variables needed for OED
eta <- eta_normalizer(design_vars) ## designs to be generated
y <- abd_normalizer(ABD)
Q <- ss_predicted_norm
## Invert the normalization
ss_predicted_norm <- predict(model, as.matrix(X))
ss_predicted <- Y_normalizer(ss_predicted_norm, invert = TRUE)
View(ss_predicted)
setwd("~/Code/MaterialAnalysis/MaterialAnalysis")
ls()
setwd("~/Code/MaterialAnalysis/MaterialAnalysis/R")
